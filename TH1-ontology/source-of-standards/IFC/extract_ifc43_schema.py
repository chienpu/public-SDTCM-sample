import csv
import re
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

BASE = "https://ifc43-docs.standards.buildingsmart.org/IFC/RELEASE/IFC4x3/HTML/"
TOC_URL = urljoin(BASE, "toc.html")

# === è¡Œç‚ºé–‹é—œ ===
LIMIT = 0                 # 0=å…¨éƒ¨ï¼›æ¸¬è©¦å¯è¨­ 20/50
ONLY_ENTITY = True        # åªè¼¸å‡º ENTITYï¼›è‹¥è¦å…¨é¡å‹è¨­ç‚º False
SLEEP_BETWEEN = 0.10      # è«‹æ±‚é–“éš”ï¼Œé¿å…è§¸ç™¼é™åˆ¶
MAX_RETRIES = 3

# ä¸€äº›å¸¸è¦‹çš„è¡¨é ­åˆ¥åï¼ˆçµ±ä¸€ç‚º name/type/descï¼‰
HDR_ALIASES = {
    "name": {"name", "attribute", "attr", "parameter"},
    "type": {"type", "data type", "datatype"},
    "desc": {"description", "desc", "definition", "remarks", "note"}
}

HEADERS = {
    "User-Agent": "Mozilla/5.0 (IFC4x3 schema extractor; +https://buildingsmart.org)"
}

def get_url(url, ok_status=(200,), retries=MAX_RETRIES):
    for i in range(retries):
        try:
            r = requests.get(url, headers=HEADERS, timeout=30)
            if r.status_code in ok_status:
                return r
            else:
                print(f"  âš ï¸ HTTP {r.status_code} for {url}")
        except Exception as e:
            print(f"  âš ï¸ Error {e} for {url}")
        time.sleep(0.5 + i*0.5)
    raise RuntimeError(f"Failed to fetch after {retries} retries: {url}")

def classify_page(soup):
    """
    ä¾æ“š EXPRESS å€å¡Šå…§å®¹åˆ¤å®šé é¢é¡åˆ¥ï¼šENTITY / TYPE / ENUM / SELECT / UNKNOWN
    """
    # å¸¸è¦‹ï¼šåœ¨ <pre> æˆ– <code> ä¸­æœƒå‡ºç¾ 'ENTITY IfcX' / 'TYPE IfcX = SELECT/ENUMERATION OF/â€¦'
    text_blobs = []
    for tag in soup.find_all(["pre", "code"]):
        text_blobs.append(tag.get_text(" ", strip=True))
    blob = " ".join(text_blobs).upper()

    if " ENTITY " in blob:
        return "ENTITY"
    if " TYPE " in blob:
        if " ENUMERATION OF " in blob:
            return "ENUM"
        if " SELECT " in blob:
            return "SELECT"
        return "TYPE"

    # å‚™æ´ï¼šæ‰¾é é¢æ®µè½
    page_txt = soup.get_text(" ", strip=True).upper()
    if "ENTITY" in page_txt:
        return "ENTITY"  # å‡è¨­
    return "UNKNOWN"

def normalize_header(h):
    h = h.strip().lower()
    for k, aliases in HDR_ALIASES.items():
        if h in aliases:
            return k
    return h

def parse_attributes_from_tables(soup):
    """
    å˜—è©¦å¾å„ç¨®è¡¨æ ¼æ¨£å¼æŠ½å‡ºå±¬æ€§åˆ—
    å›å‚³ list(dict(name,type,desc))
    """
    out = []
    for tbl in soup.find_all("table"):
        # æ“·å–è¡¨é ­
        headers = [normalize_header(th.get_text(" ", strip=True)) for th in tbl.find_all("th")]
        if not headers:
            continue
        # è‡³å°‘è¦åŒ…å« name èˆ‡ type å…¶ä¸€
        if not (("name" in headers) or ("type" in headers)):
            continue

        # æ‰¾å‡ºæ¬„ä½ç´¢å¼•
        try:
            idx_name = headers.index("name")
        except ValueError:
            idx_name = None
        try:
            idx_type = headers.index("type")
        except ValueError:
            idx_type = None
        # æè¿°æ¬„å¯èƒ½å«ä¸åŒåå­—ï¼ŒæŒ‘ä¸€å€‹æœ€åƒçš„
        idx_desc = None
        for key in ("desc", "description", "definition", "remarks", "note"):
            if key in headers:
                idx_desc = headers.index(key)
                break

        # é€åˆ—å–å€¼
        for tr in tbl.find_all("tr"):
            cells = [td.get_text(" ", strip=True) for td in tr.find_all(["td", "th"])]
            if len(cells) < 2:
                continue
            name_val = cells[idx_name] if idx_name is not None and idx_name < len(cells) else ""
            type_val = cells[idx_type] if idx_type is not None and idx_type < len(cells) else ""
            desc_val = cells[idx_desc] if idx_desc is not None and idx_desc < len(cells) else ""
            # éæ¿¾æ‰è¡¨é ­æˆ–ç©ºç™½åˆ—
            if name_val and name_val.lower() not in {"name", "attribute", "attr"}:
                out.append({"name": name_val, "type": type_val, "desc": desc_val})
    return out

def parse_definition(soup):
    # å„ªå…ˆæŠ“ .definitionï¼›å¦å‰‡æŠ“ç¬¬ä¸€å€‹æ®µè½å‚™æ´
    def_div = soup.find("div", {"class": "definition"})
    if def_div:
        return def_div.get_text(" ", strip=True)
    p = soup.find("p")
    return p.get_text(" ", strip=True) if p else ""

def main():
    print(f"ğŸ“‘ Scanning TOC: {TOC_URL}")
    toc = get_url(TOC_URL)
    toc_soup = BeautifulSoup(toc.text, "lxml")

    # 1) æ‰¾æ‰€æœ‰ content.html æ¨¡çµ„é 
    module_pages = []
    for a in toc_soup.select("a[href]"):
        href = a["href"]
        if href.endswith("content.html"):
            full = urljoin(BASE, href)
            if full not in module_pages:
                module_pages.append(full)
    print("ğŸ“‘ Found schema modules:", len(module_pages))
    print("ğŸ‘‰ First 5 modules:", module_pages[:5])

    # 2) åœ¨æ¯å€‹æ¨¡çµ„é æ‰¾ lexical/Ifc*.htm é€£çµ
    entity_links = []
    for page in module_pages:
        print(f"ğŸ” Scanning {page}")
        try:
            mod = get_url(page)
        except Exception as e:
            print(f"  âš ï¸ Skip module {page}: {e}")
            continue
        soup = BeautifulSoup(mod.text, "lxml")

        # é—œéµï¼šä¸é™å®šåœ¨ <td>ï¼Œç›´æ¥æŠ“ä»»ä½•å« lexical/Ifc çš„é€£çµ
        for a in soup.select('a[href*="lexical/Ifc"]'):
            text = a.get_text(" ", strip=True)
            href = a.get("href")
            if not text:
                # æœ‰äº›é é¢é€£çµçš„æ–‡å­—ä¸æ˜¯åç¨±ï¼Œå¾ href å–
                m = re.search(r"lexical/(ifc[^./]+)\.htm", href, re.I)
                text = m.group(1) if m else ""
            if text and text.startswith("Ifc"):
                full = urljoin(BASE, href)
                entity_links.append((text, full))
        time.sleep(SLEEP_BETWEEN)

    # å»é‡
    seen = set()
    entity_links = [(n, u) for (n, u) in entity_links if (n, u) not in seen and not seen.add((n, u))]
    print("ğŸ”¢ Found total lexical pages:", len(entity_links))
    print("ğŸ‘‰ First 10:", entity_links[:10])

    # å¯å…ˆé™åˆ¶æ•¸é‡æ¸¬è©¦
    links_to_fetch = entity_links if LIMIT == 0 else entity_links[:LIMIT]

    # 3) é€é è§£æ
    rows = []
    kept = 0
    for name, url in links_to_fetch:
        try:
            r = get_url(url)
            soup = BeautifulSoup(r.text, "lxml")
            kind = classify_page(soup)
            if ONLY_ENTITY and kind != "ENTITY":
                # ç•¥é TYPE/ENUM/SELECT
                continue

            definition = parse_definition(soup)
            attrs = parse_attributes_from_tables(soup)
            kept += 1
            print(f"âœ… [{kind}] {name} â€” {len(attrs)} attributes")

            if attrs:
                for a in attrs:
                    rows.append({
                        "Entity": name,
                        "Kind": kind,
                        "Definition": definition,
                        "Attr_Name": a["name"],
                        "Attr_Type": a["type"],
                        "Attr_Desc": a["desc"],
                        "Source": url
                    })
            else:
                # æ²’æœ‰å±¬æ€§ä¹Ÿä¿ç•™ä¸€åˆ—ï¼ˆDefinition ä»ç„¶æœ‰ç”¨ï¼‰
                rows.append({
                    "Entity": name,
                    "Kind": kind,
                    "Definition": definition,
                    "Attr_Name": "",
                    "Attr_Type": "",
                    "Attr_Desc": "",
                    "Source": url
                })
        except Exception as e:
            print(f"  âŒ Error parsing {name} -> {e}")
        time.sleep(SLEEP_BETWEEN)

    # 4) è¼¸å‡º CSV
    out = "IFC4x3_full_schema.csv"
    with open(out, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(
            f,
            fieldnames=["Entity", "Kind", "Definition", "Attr_Name", "Attr_Type", "Attr_Desc", "Source"]
        )
        writer.writeheader()
        writer.writerows(rows)

    print(f"ğŸ‰ Exported {len(rows)} rows from {kept} pages to {out}")

if __name__ == "__main__":
    main()
